{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-01T12:37:50.363966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Libraries\n\nimport pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#plt.style.use('seaborn-dark')\n\n#DateTime\nimport datetime as dt\n\n#Models\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\n\n#Sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#Time to run Program\nimport time ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"def load_data():\n  '''\n  Function to Load the Train, Test and Submission Data\n\n  returns: train, test, submission dataframes\n  '''  \n\n  train = pd.read_csv('/kaggle/input/widsdatathon2023/train_data.csv')\n  test = pd.read_csv('/kaggle/input/widsdatathon2023/test_data.csv')\n  submission = pd.read_csv('/kaggle/input/widsdatathon2023/sample_solution.csv')\n\n  return train, test, submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Declare Target and Feature\nTARGET = 'contest-tmp2m-14d__tmp2m'\nfeature = ['date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test, submission = load_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RMSE\ndef rmse():\n  y_pred = train.iloc[10000:11322, 2]\n  y = train.iloc[10000:11322, 0]\n  metric = np.sqrt(mean_squared_error(y, y_pred))\n  print(f\"RMSE of Data is: {metric}\")\n\n#Hackathon Metric\ndef predict(model, model_features):\n  pred_train = model.predict(X_train[model_features])\n  pred_val = model.predict(X_val[model_features])\n\n  print(f\"Train RMSE = {np.sqrt(mean_squared_error(y_train, pred_train))}\")\n  print(f\"Test RMSE = {np.sqrt(mean_squared_error(y_val, pred_val))}\")\n\ndef run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[TARGET]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = metric(y_val, preds_val)\n    print(f'\\nRMSE score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = metric(target, oofs)\n  print(f'\\n\\nRMSE for oofs is {oofs_score}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(12, 6))\n\n  return oofs, preds, fi\n\ndef metric(y_true, y_pred):\n  return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef download_preds(preds_test, file_name = 'hacklive_sub.csv'):\n\n  ## 1. Setting the target column with our obtained predictions\n  submission['prediction'] = preds_test\n\n  ## 2. Saving our predictions to a csv file\n\n  submission.to_csv(file_name, index = False)\n\n  ## 3. Downloading and submitting the csv file\n  from google.colab import files\n  files.download(file_name)\n\n#Download Submission File\ndef download(model, model_features, file_name = 'prophet.csv'):\n\n  pred_test = model.predict(model_features)\n\n  #Setting the target column with our obtained predictions\n  submission['prediction'] = pred_test\n\n  #Saving our predictions to a csv file\n  submission.to_csv(file_name, index = False)\n  \n  #Downloadingthe csv file\n  files.download(file_name)\n\ndef join_df(train, test):\n\n  df = pd.concat([train, test], axis=0).reset_index(drop = True)\n  features = [c for c in df.columns if c not in [feature, TARGET]]\n  df[TARGET] = df[TARGET].apply(lambda x: np.log1p(x))\n\n  return df, features\n\ndef split_df_and_get_features(df, train_nrows):\n\n  train, test = df[:train_nrows].reset_index(drop = True), df[train_nrows:].reset_index(drop = True)\n  features = [c for c in train.columns if c not in [feature, TARGET]]\n  \n  return train, test, features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA and Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#Combine Train and Test Dataframe\ndf, features = join_df(train, test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Details","metadata":{}},{"cell_type":"code","source":"print(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check Datatypes\ntrain.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null Values","metadata":{}},{"cell_type":"code","source":"print(f\"Train Null Value Count: {train.isnull().sum()}\")\nprint(f\"Test Null Value Count: {test.isnull().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Distribution","metadata":{}},{"cell_type":"code","source":"#Temperature Distribution\ntrain[TARGET].plot(kind = 'density', title = 'Temperature Distribution', fontsize=14, figsize=(10, 6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Log Temperature Distribution\n_ = pd.Series(np.log1p(train[TARGET])).plot(kind = 'density', title = 'Log Temperature Distribution', fontsize=14, figsize=(10, 6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Temperature Boxplot\ntrain[TARGET].plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Temperature Boxplot', fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Log Temperature BoxPlot\npd.Series(np.log1p(train[TARGET])).plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Log Temperature Boxplot', fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Date Feature","metadata":{}},{"cell_type":"code","source":"#Convert `date` column datatype to `datetime`\ndf['startdate'] = pd.to_datetime(df['startdate'])\n\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train Null Value Count: {train.isnull().sum()}\")\nprint(f\"Test Null Value Count: {test.isnull().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make basic datetime features\n# df['day_of_week'] = df['date'].dt.dayofweek\ndf['year'] = df['startdate'].dt.year\ndf['month'] = df['startdate'].dt.month\ndf['week'] = df['startdate'].dt.isocalendar().week  \n\n#Get Train and Test sets from df\ntrain, test, features = split_df_and_get_features(df, train.shape[0])\n\n#Define the features\nfeatures = [c for c in df.columns if c not in [feature, TARGET]]\nfeatures = features[1:]\nfeatures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"#Declare Features and Target from Training Dataset\nX = train[features]\ny = train[TARGET]\n\n#Split Training and Validation Datasets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LGBMRegressor\nmodel = LGBMRegressor(n_estimators = 5000,\n                        learning_rate = 0.01,\n                        colsample_bytree = 0.76,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds, fi = run_gradient_boosting(clf = model, fit_params = fit_params, train = train, test = test, features = features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Series Forecasting using ARIMA and SARIMAX\n\n","metadata":{}},{"cell_type":"markdown","source":"### Preprocess Data","metadata":{}},{"cell_type":"code","source":"#Load Data\ntrain, test, submission = load_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert `date` column to datetime\ntrain.startdate = pd.to_datetime(train.startdate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Data","metadata":{}},{"cell_type":"code","source":"train.plot(figsize = (20, 10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make Data Stationary","metadata":{}},{"cell_type":"code","source":"#Import adfuller test\nfrom statsmodels.tsa.stattools import adfuller","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#H0: It is not stationary\n#H1: It is stationary\n\ndef adfuller_test(temp):\n    result=adfuller(temp)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adfuller_test(train['contest-tmp2m-14d__tmp2m'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Seasonal First Difference']=train['contest-tmp2m-14d__tmp2m']-train['contest-tmp2m-14d__tmp2m'].shift(12) #Because 1 year has 12 months\n\n## Again test dickey fuller test\nadfuller_test(train['Seasonal First Difference'].dropna())\n\ntrain['Seasonal First Difference'].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf ,plot_acf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (12, 8))\nax1 = fig.add_subplot(211)\nfig = plot_pacf(train['Seasonal First Difference'].iloc[13:],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = plot_acf(train['Seasonal First Difference'].iloc[13:],lags=40,ax=ax2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from statsmodels.tsa.arima_model import ARIMA\n\nmodel=sm.tsa.statespace.ARIMA(train['contest-tmp2m-14d__tmp2m'],order=(2,0,2))\nmodel_fit=model.fit()\n\nmodel_fit.summary()\n\ntrain['forecast']=model_fit.predict(start=10000,end=11321,dynamic=True)\ntrain[['contest-tmp2m-14d__tmp2m','forecast']].plot(figsize=(12,8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\n\n#Start time \nbegin = time.time() \n\nmodel=sm.tsa.statespace.SARIMAX(train['contest-tmp2m-14d__tmp2m'],order=(2, 1, 2),seasonal_order=(2, 1, 2, 12))\nresults=model.fit()\n\n#End TIme\nend = time.time()\nprint(f\"\\n\\nTime of execution = {end - begin}\")\n\n#Forecast\ntrain['forecast']=results.predict(start=10000,end=11321,dynamic=True)\ntrain[['contest-tmp2m-14d__tmp2m','forecast']].plot(figsize=(12,8))\n\nrmse()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train, test])\ndf['forecast'] = results.predict(start = 11322, end = 14883, dynamic= True)  \ndf[['contest-tmp2m-14d__tmp2m', 'forecast']].plot(figsize=(12, 8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}